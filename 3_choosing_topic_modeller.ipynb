{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5d82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c8c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f946b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA visualization \n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a5c03",
   "metadata": {},
   "source": [
    "We know we will be using LDA because our text is long, but let's give it a try with the other topic modellers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac227b1",
   "metadata": {},
   "source": [
    "## Load data + create Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08897c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pd.read_csv(\"ted_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01b6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = ted.transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f29e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer(stop_words = 'english')\n",
    "dtm_tf = tf.fit_transform(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36d44e",
   "metadata": {},
   "source": [
    "We'll go wit a default of 10 topics and try the 3 models of: LDA, NMF & LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d70f96",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b94475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_tf.fit(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f140606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      " like, earth, light, just, time, universe, years, life, know, space, planet, look, way, ocean, actually \n",
      "\n",
      "Topic 1:\n",
      " music, sound, like, laughter, language, applause, going, just, know, people, think, play, way, sounds, right \n",
      "\n",
      "Topic 2:\n",
      " women, like, men, just, laughter, time, know, work, did, people, think, going, really, world, make \n",
      "\n",
      "Topic 3:\n",
      " like, just, laughter, going, really, know, did, think, time, little, way, things, got, people, thing \n",
      "\n",
      "Topic 4:\n",
      " brain, cells, health, cancer, body, disease, patients, like, blood, medical, heart, care, patient, just, time \n",
      "\n",
      "Topic 5:\n",
      " people, think, like, just, going, really, actually, know, things, want, right, way, time, make, data \n",
      "\n",
      "Topic 6:\n",
      " like, just, water, world, really, going, new, years, make, need, actually, use, people, think, energy \n",
      "\n",
      "Topic 7:\n",
      " people, world, country, years, countries, just, percent, africa, new, need, today, like, government, time, states \n",
      "\n",
      "Topic 8:\n",
      " people, know, like, said, laughter, just, did, time, going, life, kids, want, school, years, got \n",
      "\n",
      "Topic 9:\n",
      " like, people, did, said, life, know, just, say, world, time, love, laughter, man, story, women \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf.get_feature_names()\n",
    "for idx, topic in enumerate(lda_tf.components_):\n",
    "    # Select the top 15 words in vocab for this topic.\n",
    "    top_words = [vocab[i] for i in topic.argsort()[:-16:-1]]\n",
    "    print(f\"Topic {idx}:\\n\", \", \".join(top_words), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599015aa",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eec973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd21673c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "doc_topic = nmf.fit_transform(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3effda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms(topic, n_terms, nmf=nmf, terms=vocab):\n",
    "    components = nmf.components_[topic, :]\n",
    "    top_term_indices = components.argsort()[-n_terms:]    \n",
    "    top_terms = np.array(terms)[top_term_indices]    \n",
    "    return top_terms.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6df6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for topic 0\n",
      "['want', 'like', 'actually', 'things', 'mean', 'lot', 'way', 'say', 'people', 'right', 'really', 'just', 'world', 'ca', 'think']\n",
      "\n",
      "for topic 1\n",
      "['better', 'change', 'health', 'need', 'percent', 'help', 'money', 'make', 'things', 'person', 'want', 'social', 'work', 'like', 'people']\n",
      "\n",
      "for topic 2\n",
      "['thought', 'come', 'kids', 'story', 'came', 'got', 'children', 'went', 'school', 'day', 'years', 'life', 'time', 'did', 'said']\n",
      "\n",
      "for topic 3\n",
      "['country', '000', 'time', 'global', 'energy', 'year', 'change', 'countries', 'today', 'water', 'new', 'percent', 'need', 'years', 'world']\n",
      "\n",
      "for topic 4\n",
      "['space', 'earth', 'make', 'light', 'music', 'look', 'little', 'way', 'time', 'life', 'kind', 'right', 'know', 'just', 'like']\n",
      "\n",
      "for topic 5\n",
      "['blood', 'right', 'sleep', 'time', 'does', 'going', 'disease', 'neurons', 'cell', 'different', 'human', 'body', 'cancer', 'cells', 'brain']\n",
      "\n",
      "for topic 6\n",
      "['little', 'yeah', 'say', 'does', 'want', 'got', 'said', 'ok', 'did', 'right', 'good', 'just', 'like', 'applause', 'laughter']\n",
      "\n",
      "for topic 7\n",
      "['know', 'male', 'talk', 'need', 'young', 'female', 'man', 'work', 'gender', 'sex', 'applause', 'girls', 'woman', 'men', 'women']\n",
      "\n",
      "for topic 8\n",
      "['kind', 'use', 'new', 'work', 'thing', 'data', 'way', 'make', 'little', 'time', 'going', 'just', 'things', 'really', 'actually']\n",
      "\n",
      "for topic 9\n",
      "['need', 'little', 'lot', 'think', 'people', 'say', 'just', 'thing', 'right', 'things', 'got', 'want', 'really', 'going', 'know']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"for topic {i}\")\n",
    "    print(get_top_terms(i, 15))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355e603",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe9cb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08749042, 0.01930134, 0.01663689, 0.01296377, 0.00974322,\n",
       "       0.00936043, 0.0084517 , 0.00812133, 0.00731244, 0.00697759])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(10)\n",
    "doc_topic = lsa.fit_transform(dtm_tf)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf8b3a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>000000004</th>\n",
       "      <th>0000001</th>\n",
       "      <th>000001</th>\n",
       "      <th>00001</th>\n",
       "      <th>000042</th>\n",
       "      <th>0001</th>\n",
       "      <th>00046</th>\n",
       "      <th>...</th>\n",
       "      <th>عسل</th>\n",
       "      <th>مسكين</th>\n",
       "      <th>مطعم</th>\n",
       "      <th>وله</th>\n",
       "      <th>อย</th>\n",
       "      <th>อยman</th>\n",
       "      <th>อร</th>\n",
       "      <th>你会说中文吗</th>\n",
       "      <th>你好</th>\n",
       "      <th>送你葱</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_6</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_8</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_9</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_10</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 68209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 00    000  0000  000000004  0000001  000001  00001  000042  \\\n",
       "component_1   0.000  0.038   0.0        0.0      0.0     0.0    0.0     0.0   \n",
       "component_2  -0.000  0.030  -0.0       -0.0     -0.0    -0.0   -0.0     0.0   \n",
       "component_3   0.000 -0.042  -0.0       -0.0      0.0    -0.0   -0.0     0.0   \n",
       "component_4  -0.000 -0.072   0.0       -0.0      0.0     0.0   -0.0    -0.0   \n",
       "component_5   0.000  0.011  -0.0        0.0     -0.0    -0.0   -0.0    -0.0   \n",
       "component_6  -0.000 -0.028   0.0       -0.0     -0.0    -0.0    0.0    -0.0   \n",
       "component_7   0.001 -0.005  -0.0        0.0     -0.0     0.0    0.0    -0.0   \n",
       "component_8  -0.000 -0.062  -0.0       -0.0     -0.0     0.0   -0.0     0.0   \n",
       "component_9  -0.000 -0.003   0.0       -0.0     -0.0     0.0    0.0    -0.0   \n",
       "component_10 -0.001 -0.003  -0.0       -0.0     -0.0     0.0    0.0     0.0   \n",
       "\n",
       "              0001  00046  ...  عسل  مسكين  مطعم  وله   อย  อยman   อร  \\\n",
       "component_1    0.0    0.0  ...  0.0    0.0   0.0  0.0  0.0    0.0  0.0   \n",
       "component_2   -0.0   -0.0  ... -0.0   -0.0  -0.0 -0.0 -0.0   -0.0 -0.0   \n",
       "component_3   -0.0    0.0  ... -0.0   -0.0  -0.0 -0.0 -0.0   -0.0 -0.0   \n",
       "component_4    0.0   -0.0  ... -0.0   -0.0  -0.0 -0.0  0.0    0.0  0.0   \n",
       "component_5    0.0   -0.0  ...  0.0    0.0   0.0  0.0 -0.0   -0.0 -0.0   \n",
       "component_6    0.0   -0.0  ... -0.0   -0.0  -0.0 -0.0  0.0    0.0  0.0   \n",
       "component_7    0.0   -0.0  ...  0.0    0.0   0.0  0.0  0.0    0.0  0.0   \n",
       "component_8    0.0   -0.0  ... -0.0   -0.0  -0.0 -0.0  0.0    0.0  0.0   \n",
       "component_9    0.0    0.0  ...  0.0    0.0   0.0  0.0  0.0    0.0  0.0   \n",
       "component_10   0.0   -0.0  ...  0.0    0.0   0.0  0.0 -0.0   -0.0 -0.0   \n",
       "\n",
       "              你会说中文吗   你好  送你葱  \n",
       "component_1      0.0  0.0  0.0  \n",
       "component_2      0.0  0.0  0.0  \n",
       "component_3     -0.0 -0.0  0.0  \n",
       "component_4     -0.0 -0.0 -0.0  \n",
       "component_5     -0.0 -0.0  0.0  \n",
       "component_6      0.0  0.0  0.0  \n",
       "component_7     -0.0 -0.0  0.0  \n",
       "component_8     -0.0 -0.0 -0.0  \n",
       "component_9     -0.0 -0.0 -0.0  \n",
       "component_10    -0.0 -0.0  0.0  \n",
       "\n",
       "[10 rows x 68209 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\", \"component_4\", \"component_5\", \\\n",
    "                     \"component_6\",\"component_7\", \"component_8\", \"component_9\", \"component_10\"],\n",
    "             columns = tf.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beca6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fedba72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "people, like, just, know, going, think, really, laughter, time, world, way, actually, did, things, want\n",
      "\n",
      "Topic  1\n",
      "people, world, countries, percent, need, country, think, health, social, change, china, global, ca, today, government\n",
      "\n",
      "Topic  2\n",
      "laughter, said, people, did, women, know, men, say, love, school, applause, got, went, day, man\n",
      "\n",
      "Topic  3\n",
      "think, people, really, going, know, ca, things, actually, lot, kind, right, thing, mean, say, sort\n",
      "\n",
      "Topic  4\n",
      "going, know, said, think, women, got, really, need, ca, want, world, say, did, years, countries\n",
      "\n",
      "Topic  5\n",
      "brain, women, cells, cancer, body, men, disease, patients, health, children, life, said, heart, blood, time\n",
      "\n",
      "Topic  6\n",
      "like, know, people, did, life, kind, time, kids, school, city, started, water, said, story, just\n",
      "\n",
      "Topic  7\n",
      "women, like, think, world, men, ca, love, woman, just, know, feel, say, mean, sex, life\n",
      "\n",
      "Topic  8\n",
      "women, actually, really, work, data, men, make, use, new, design, like, health, need, woman, working\n",
      "\n",
      "Topic  9\n",
      "like, know, going, water, need, right, health, food, percent, brain, people, care, cancer, let, disease\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, tf.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4742d4e",
   "metadata": {},
   "source": [
    "Definitely LDA is the leader here but I'd say NMF is a second best and LSA is a bit behind. \\\n",
    "And I'd say 'like' is definitely a stop_word :) We'll deal with that in the next chapter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
